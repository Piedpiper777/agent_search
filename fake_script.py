from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any, Callable
from langgraph.graph import StateGraph, END
import uuid
import time
import functools
from openai import OpenAI  # æˆ–å…¶ä»–LLMå®¢æˆ·ç«¯

# ===== 1. æ¶ˆæ¯ç»“æ„ =====
@dataclass
class Message:
    role: str
    action: str
    content: Any
    metadata: Dict[str, Any] = field(default_factory=dict)
    session_id: str = field(default_factory=lambda: str(uuid.uuid4()))  # æ·»åŠ session_idç”¨äºè¿½è¸ª
    timestamp: float = field(default_factory=time.time)  # æ·»åŠ æ—¶é—´æˆ³ä¾¿äºè°ƒè¯•å’Œåˆ†æ

# ===== 2. AgentçŠ¶æ€ =====
@dataclass
class AgentState:
    query: str
    history: List[Message] = field(default_factory=list)
    current_message: Optional[Message] = None
    collected_facts: List[Dict] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def add_message(self, msg: Message) -> None:
        self.current_message = msg
        self.history.append(msg)
        
    def get_facts_by_source(self, source: str) -> List[Dict]:
        """è·å–ç‰¹å®šæ¥æºçš„facts"""
        return [f for f in self.collected_facts if f["source"] == source]
    
    def get_last_message_by_role(self, role: str) -> Optional[Message]:
        """è·å–æœ€è¿‘çš„ç‰¹å®šè§’è‰²æ¶ˆæ¯"""
        return next((m for m in reversed(self.history) if m.role == role), None)

# ===== 3. å·¥å…·å®ç° =====
def decompose_tool(query: str) -> Dict[str, Any]:
    """æŸ¥è¯¢åˆ†è§£å·¥å…·,ä½¿ç”¨LLMå°†å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸ºå­æŸ¥è¯¢"""
    # å®é™…å®ç°ä¸­æ›¿æ¢ä¸ºçœŸå®çš„LLMè°ƒç”¨
    return {
        "subqueries": [
            {
                "id": "sub1",
                "query": "What movies did Nolan direct between 2010-2020?",
                "reasoning": "åˆ†è§£æ—¶é—´èŒƒå›´,ä¾¿äºç²¾ç¡®æ£€ç´¢"
            },
            {
                "id": "sub2", 
                "query": "What are Nolan's most recent films?",
                "reasoning": "è¡¥å……æœ€æ–°ä¿¡æ¯"
            }
        ],
        "strategy": "æŒ‰æ—¶é—´ç»´åº¦æ‹†åˆ†,ç¡®ä¿ä¿¡æ¯å®Œæ•´æ€§",
        "original_query": query
    }

def search_kg_tool(query: str) -> Dict[str, Any]:
    """çŸ¥è¯†å›¾è°±æœç´¢å·¥å…·,å†…éƒ¨åŒ…å«å®Œæ•´çš„æ£€ç´¢é“¾è·¯"""
    
    def _align(query: str) -> Dict[str, Any]:
        """å†…éƒ¨aligner,å°†æŸ¥è¯¢å¯¹é½åˆ°å›¾è°±æ¨¡å¼"""
        # å®é™…å®ç°ä¸­æ›¿æ¢ä¸ºçœŸå®çš„LLMè°ƒç”¨
        aligned_query = f"aligned: {query}"
        return {
            "kg_query": aligned_query,
            "confidence": 0.9,
            "metadata": {
                "reasoning": "å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºå›¾è°±æŸ¥è¯¢æ¨¡å¼",
                "original_query": query
            }
        }
    
    def _execute(kg_query: str) -> Dict[str, Any]:
        """å†…éƒ¨executor,æ‰§è¡Œå›¾è°±æŸ¥è¯¢"""
        facts = [
            {"fact": "fact1", "confidence": 0.9, "source": "KG_Path_1"},
            {"fact": "fact2", "confidence": 0.85, "source": "KG_Path_2"}
        ]
        return {
            "facts": facts,
            "confidence": sum(f["confidence"] for f in facts) / len(facts),
            "metadata": {
                "kg_query": kg_query,
                "paths_searched": ["KG_Path_1", "KG_Path_2"]
            }
        }
    
    def _discriminate(facts: List[Dict], query: str) -> Dict[str, Any]:
        """å†…éƒ¨discriminator,è¯„ä¼°æ£€ç´¢ç»“æœè´¨é‡"""
        # å®é™…å®ç°ä¸­æ›¿æ¢ä¸ºçœŸå®çš„LLMè°ƒç”¨
        scores = [
            {
                "fact": fact["fact"],
                "relevance": 0.8,
                "reliability": 0.85,
                "completeness": 0.75
            }
            for fact in facts
        ]
        
        avg_score = sum(
            (s["relevance"] + s["reliability"] + s["completeness"]) / 3 
            for s in scores
        ) / len(scores)
        
        return {
            "score": avg_score,
            "fact_scores": scores,
            "metadata": {
                "query": query,
                "analysis": "è¯¦ç»†åˆ†ææ¯ä¸ªfactçš„ç›¸å…³æ€§ã€å¯é æ€§å’Œå®Œæ•´æ€§"
            }
        }

    # å†…éƒ¨æ£€ç´¢é“¾è·¯æ‰§è¡Œ
    try:
        # 1. æŸ¥è¯¢å¯¹é½
        align_result = _align(query)
        kg_query = align_result["kg_query"]
        
        # 2. æ‰§è¡ŒæŸ¥è¯¢
        execute_result = _execute(kg_query)
        facts = execute_result["facts"]
        
        # 3. ç»“æœè¯„ä¼°
        evaluate_result = _discriminate(facts, query)
        
        # 4. æ•´åˆç»“æœ
        return {
            "facts": facts,
            "confidence": execute_result["confidence"],
            "quality_score": evaluate_result["score"],
            "metadata": {
                "query": query,
                "kg_query": kg_query,
                "align_confidence": align_result["confidence"],
                "fact_analysis": evaluate_result["fact_scores"],
                "execution_metadata": execute_result["metadata"]
            }
        }
    
    except Exception as e:
        # å¼‚å¸¸å¤„ç†
        return {
            "facts": [],
            "confidence": 0.0,
            "error": str(e),
            "metadata": {
                "query": query,
                "error_type": type(e).__name__
            }
        }

def search_vector_tool(query: str) -> Dict[str, Any]:
    """å‘é‡æ•°æ®åº“æœç´¢å·¥å…·,å†…éƒ¨åŒ…å«å®Œæ•´çš„æ£€ç´¢é“¾è·¯"""
    
    def _embed(query: str) -> Dict[str, Any]:
        """å†…éƒ¨embedding,å°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡"""
        # å®é™…å®ç°ä¸­æ›¿æ¢ä¸ºçœŸå®çš„embeddingæ¨¡å‹è°ƒç”¨
        return {
            "vector": [0.1, 0.2, 0.3],  # ç¤ºä¾‹å‘é‡
            "confidence": 0.95,
            "metadata": {
                "model": "text-embedding-ada-002",
                "original_query": query
            }
        }
    
    def _search(vector: List[float]) -> Dict[str, Any]:
        """å†…éƒ¨vector search,æ‰§è¡Œå‘é‡æ£€ç´¢"""
        facts = [
            {"fact": "vector fact 1", "similarity": 0.92, "source": "Doc_1"},
            {"fact": "vector fact 2", "similarity": 0.88, "source": "Doc_2"}
        ]
        return {
            "facts": facts,
            "confidence": sum(f["similarity"] for f in facts) / len(facts),
            "metadata": {
                "top_k": 2,
                "index": "main_docs"
            }
        }
    
    def _rerank(facts: List[Dict], query: str) -> Dict[str, Any]:
        """å†…éƒ¨reranker,é‡æ’åºæ£€ç´¢ç»“æœ"""
        # å®é™…å®ç°ä¸­æ›¿æ¢ä¸ºçœŸå®çš„rerankeræ¨¡å‹è°ƒç”¨
        scores = [
            {
                "fact": fact["fact"],
                "relevance": 0.9,
                "semantic_similarity": 0.85
            }
            for fact in facts
        ]
        
        avg_score = sum(
            (s["relevance"] + s["semantic_similarity"]) / 2 
            for s in scores
        ) / len(scores)
        
        return {
            "score": avg_score,
            "fact_scores": scores,
            "metadata": {
                "query": query,
                "rerank_model": "cross-encoder"
            }
        }

    try:
        # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
        embed_result = _embed(query)
        query_vector = embed_result["vector"]
        
        # 2. æ‰§è¡Œå‘é‡æ£€ç´¢
        search_result = _search(query_vector)
        facts = search_result["facts"]
        
        # 3. ç»“æœé‡æ’åº
        rerank_result = _rerank(facts, query)
        
        # 4. æ•´åˆç»“æœ
        return {
            "facts": facts,
            "confidence": search_result["confidence"],
            "quality_score": rerank_result["score"],
            "metadata": {
                "query": query,
                "embedding_confidence": embed_result["confidence"],
                "fact_analysis": rerank_result["fact_scores"],
                "search_metadata": search_result["metadata"],
                "rerank_metadata": rerank_result["metadata"]
            }
        }
    
    except Exception as e:
        # å¼‚å¸¸å¤„ç†
        return {
            "facts": [],
            "confidence": 0.0,
            "error": str(e),
            "metadata": {
                "query": query,
                "error_type": type(e).__name__
            }
        }

def generate_answer_tool(context: Dict[str, Any]) -> Dict[str, Any]:
    """ç­”æ¡ˆç”Ÿæˆå·¥å…·,ä½¿ç”¨LLMåŸºäºæ”¶é›†çš„ä¿¡æ¯ç”Ÿæˆç­”æ¡ˆ"""
    # contextåŒ…å«:åŸå§‹queryã€åˆ†è§£strategyã€å­æŸ¥è¯¢åŠæ¨ç†è¿‡ç¨‹ã€æ£€ç´¢åˆ°çš„facts
    return {
        "answer": "æ ¹æ®è°ƒç ”,Christopher Nolanåœ¨2010-2020æœŸé—´æ‰§å¯¼äº†...",
        "confidence": 0.95,
        "reasoning_chain": [
            "1. åˆ†æäº†ä¸åŒæ—¶æœŸçš„ä½œå“",
            "2. æ•´åˆäº†æœ€æ–°ä¿¡æ¯",
            "3. å½¢æˆå®Œæ•´ç­”æ¡ˆ"
        ]
    }

# ===== 4. å·¥å…·æ³¨å†Œè¡¨ =====
TOOL_REGISTRY = {
    "decompose": {
        "name": "decompose",
        "description": "å°†å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸ºå¤šä¸ªç®€å•æŸ¥è¯¢,å¹¶æä¾›åˆ†è§£æ€è·¯",
        "fn": decompose_tool,
        "required_fields": ["query"],
        "optional_fields": []
    },
    "search_kg": {
        "name": "search_kg",
        "description": "åœ¨çŸ¥è¯†å›¾è°±ä¸­æœç´¢ç›¸å…³äº‹å®",
        "fn": search_kg_tool,
        "required_fields": ["query"],
        "optional_fields": []
    },
    "search_vector": {
        "name": "search_vector",
        "description": "åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢ç›¸ä¼¼å†…å®¹",
        "fn": search_vector_tool,  # ä½¿ç”¨å®Œæ•´çš„å·¥å…·å®ç°
        "required_fields": ["query"],
        "optional_fields": []
    },
    "generate": {
        "name": "generate",
        "description": "åŸºäºæ”¶é›†çš„ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ",
        "fn": generate_answer_tool,
        "required_fields": ["context"],
        "optional_fields": []
    }
}

def validate_tool_input(tool_name: str, input_data: Any) -> bool:
    """å·¥å…·è¾“å…¥éªŒè¯"""
    tool = TOOL_REGISTRY[tool_name]
    if isinstance(input_data, dict):
        return all(field in input_data for field in tool["required_fields"])
    return isinstance(input_data, str)  # é»˜è®¤æ¥å—å­—ç¬¦ä¸²è¾“å…¥

# ===== 5. Controlleré€»è¾‘ =====
def get_llm_decision(prompt: str) -> Dict[str, Any]:
    """è°ƒç”¨LLMè·å–å†³ç­–"""
    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{
            "role": "system",
            "content": """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½Agentçš„æ§åˆ¶å™¨ã€‚
ä½ éœ€è¦åŸºäºå½“å‰çŠ¶æ€å†³å®šä¸‹ä¸€æ­¥æ“ä½œã€‚
å¿…é¡»ä»¥JSONæ ¼å¼å›å¤ï¼ŒåŒ…å«:
{
    "thought": "æ¨ç†è¿‡ç¨‹",
    "action": "è¦ä½¿ç”¨çš„å·¥å…·å",
    "input": "å·¥å…·è¾“å…¥",
    "reason": "é€‰æ‹©åŸå› "
}"""
        }, {
            "role": "user",
            "content": prompt
        }],
        temperature=0.7,
        response_format={ "type": "json_object" }
    )
    return response.choices[0].message.content

def controller_node(state: AgentState) -> AgentState:
    """åŠ¨æ€å†³ç­–æ§åˆ¶å™¨"""
    
    # 1. æ„å»ºå·¥å…·æè¿°
    tools_desc = "\n".join(
        f"- {name}: {info['description']}\n  æ‰€éœ€å‚æ•°: {info['required_fields']}"
        for name, info in TOOL_REGISTRY.items()
    )
    
    # 2. æ„å»ºæ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
    facts_summary = "\n".join(
        f"- Source: {fact['source']}\n  Facts: {fact['facts']}\n  Confidence: {fact.get('confidence', 'N/A')}"
        for fact in state.collected_facts
    )
    
    history_summary = "\n".join(
        f"Step {i}: {msg.role} -> {msg.action}\n  Result: {msg.content}\n  Thought: {msg.metadata.get('thought', 'N/A')}"
        for i, msg in enumerate(state.history)
    )
    
    current_status = {
        "facts_count": len(state.collected_facts),
        "steps_taken": len(state.history),
        "last_action": state.current_message.action if state.current_message else None,
        "last_result_quality": state.current_message.content.get("quality_score", 0) if state.current_message else 0
    }
    
    # 3. æ„å»ºæç¤ºè¯
    prompt = f"""å½“å‰æŸ¥è¯¢: {state.query}

å¯ç”¨å·¥å…·:
{tools_desc}

å½“å‰çŠ¶æ€:
- å·²æ”¶é›†Factsæ•°é‡: {current_status['facts_count']}
- å·²æ‰§è¡Œæ­¥éª¤: {current_status['steps_taken']}
- ä¸Šæ¬¡è¡ŒåŠ¨: {current_status['last_action']}
- ä¸Šæ¬¡ç»“æœè´¨é‡: {current_status['last_result_quality']}

å·²æ”¶é›†çš„Facts:
{facts_summary}

æ‰§è¡Œå†å²:
{history_summary}

è¯·åˆ†æå½“å‰çŠ¶æ€å¹¶å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨:
1. æ˜¯å¦éœ€è¦åˆ†è§£æŸ¥è¯¢?
2. æ˜¯å¦éœ€è¦æ”¶é›†æ›´å¤šä¿¡æ¯?
3. å·²æœ‰ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿç”Ÿæˆç­”æ¡ˆ?
4. æ˜¯å¦éœ€è¦å°è¯•å…¶ä»–æ£€ç´¢ç­–ç•¥?

åŸºäºä»¥ä¸Šåˆ†æ,ç»™å‡ºä½ çš„å†³ç­–ã€‚"""

    try:
        # 3. è°ƒç”¨LLMè·å–å†³ç­–
        decision = get_llm_decision(prompt)
        
        # 4. éªŒè¯å†³ç­–åˆæ³•æ€§
        if decision["action"] not in TOOL_REGISTRY and decision["action"] != "generate":
            raise ValueError(f"Invalid action: {decision['action']}")
        
        if not validate_tool_input(decision["action"], decision["input"]):
            raise ValueError(f"Invalid input for tool: {decision['action']}")
        
        # 5. æ‰§è¡Œå†³ç­–é€»è¾‘
        if decision["action"] == "generate":
            # æ„å»ºç”Ÿæˆå™¨æ‰€éœ€çš„å®Œæ•´ä¸Šä¸‹æ–‡
            generation_context = {
                "original_query": state.query,
                "decomposition": next(
                    (msg.content for msg in state.history 
                     if msg.role == "decompose"), 
                    None
                ),
                "collected_facts": state.collected_facts,
            }
            result = TOOL_REGISTRY["generate"]["fn"](generation_context)
            
            msg = Message(
                role="generator",
                action="generate",
                content=result,
                metadata={"thought": decision["thought"]}
            )
            state.add_message(msg)
            return END
        else:
            tool = TOOL_REGISTRY[decision["action"]]
            result = tool["fn"](decision["input"])
            
            msg = Message(
                role=tool["name"],
                action=decision["action"],
                content=result,
                metadata={
                    "thought": decision["thought"],
                    "reason": decision["reason"]
                }
            )
            state.add_message(msg)
            
            # å¦‚æœè·å–äº†æ–°facts,ä¿å­˜èµ·æ¥
            if "facts" in result:
                state.collected_facts.append({
                    "source": decision["action"],
                    "facts": result["facts"],
                    "metadata": result.get("metadata", {})
                })
    
        return state
        
    except Exception as e:
        state.metadata["last_error"] = {
            "error": str(e),
            "context": "controller_decision",
            "prompt": prompt
        }
        return state

# ===== 6. å·¥å…·èŠ‚ç‚¹åŒ…è£…å™¨ =====
def tool_node(state: AgentState, tool_name: str) -> AgentState:
    """å·¥å…·èŠ‚ç‚¹çš„ç»Ÿä¸€åŒ…è£…å™¨"""
    try:
        tool = TOOL_REGISTRY[tool_name]
        result = tool["fn"](state.current_message.content)
        
        msg = Message(
            role=tool["name"],
            action=state.current_message.action,
            content=result,
            metadata=state.current_message.metadata
        )
        state.add_message(msg)
        
        if "facts" in result:
            state.collected_facts.append({
                "source": tool_name,
                "facts": result["facts"],
                "metadata": result.get("metadata", {})
            })
        
        # è®°å½•æˆåŠŸçŠ¶æ€
        state.metadata["last_error"] = None
        return state
        
    except Exception as e:
        # è®°å½•é”™è¯¯ä¿¡æ¯
        state.metadata["last_error"] = {
            "tool": tool_name,
            "error": str(e),
            "error_type": type(e).__name__
        }
        return state

# ===== 7. å›¾æ„å»º =====
def build_graph() -> StateGraph:
    graph = StateGraph(AgentState)
    
    # 1. æ·»åŠ èŠ‚ç‚¹
    graph.add_node("controller", controller_node)
    graph.add_node("error_handler", error_handler_node)
    
    # 2. æ·»åŠ å·¥å…·èŠ‚ç‚¹
    for tool_name in TOOL_REGISTRY:
        graph.add_node(tool_name, functools.partial(tool_node, tool_name=tool_name))
    
    # 3. è®¾ç½®å…¥å£ç‚¹
    graph.set_entry_point("controller")
    
    # 4. æ·»åŠ æ­£å¸¸æµç¨‹è¾¹
    for tool_name in TOOL_REGISTRY:
        graph.add_edge("controller", tool_name)
        graph.add_edge(tool_name, "controller")
    
    # 5. æ·»åŠ é”™è¯¯å¤„ç†è¾¹
    def has_error(state: AgentState) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯"""
        return state.metadata.get("last_error") is not None
    
    def no_error(state: AgentState) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ— é”™è¯¯"""
        return not has_error(state)
    
    # ä»å·¥å…·èŠ‚ç‚¹åˆ°é”™è¯¯å¤„ç†èŠ‚ç‚¹çš„æ¡ä»¶è¾¹
    for tool_name in TOOL_REGISTRY:
        graph.add_conditional_edge(
            tool_name,
            "error_handler",
            condition=has_error
        )
    
    # é”™è¯¯å¤„ç†åè¿”å›controller
    graph.add_edge("error_handler", "controller")
    
    # 6. æ·»åŠ ç»“æŸæ¡ä»¶
    def should_end(state: AgentState) -> bool:
        return (
            state.current_message and 
            (state.current_message.role == "generator" or
             len(state.history) > 10)  # æ·»åŠ æœ€å¤§æ­¥æ•°é™åˆ¶
        )
    
    graph.add_edge("controller", END, should_end)
    
    return graph.compile()

# ===== 8. è¿è¡Œç¤ºä¾‹ =====
if __name__ == "__main__":
    query = "What films did Christopher Nolan direct?"
    state = AgentState(query=query)
    app = build_graph()
    final_state = app.invoke(state)
    
    print("\nğŸ¯ Query:", query)
    print("\nğŸ“ æ‰§è¡Œå†å²:")
    for msg in final_state.history:
        print(f"- [{msg.role}] {msg.action}")
        print(f"  æ€è€ƒ: {msg.metadata.get('thought', '')}")
        print(f"  ç»“æœ: {msg.content}")

def handle_error(state: AgentState, error_msg: str) -> AgentState:
    """ç»Ÿä¸€çš„é”™è¯¯å¤„ç†"""
    error_context = {
        "original_query": state.query,
        "error": error_msg,
        "collected_facts": state.collected_facts
    }
    
    result = {
        "answer": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶é‡åˆ°äº†é—®é¢˜ã€‚åŸºäºå·²æ”¶é›†çš„ä¿¡æ¯ï¼Œæˆ‘å¯ä»¥å‘Šè¯‰æ‚¨...",
        "confidence": 0.5,
        "error": error_msg
    }
    
    msg = Message(
        role="generator",
        action="generate",
        content=result,
        metadata={"error": error_msg}
    )
    state.add_message(msg)
    return END

def error_handler_node(state: AgentState) -> AgentState:
    """é”™è¯¯å¤„ç†èŠ‚ç‚¹"""
    error_info = state.metadata.get("last_error", {})
    
    error_context = {
        "original_query": state.query,
        "error": error_info.get("error", "Unknown error"),
        "error_tool": error_info.get("tool", "Unknown tool"),
        "collected_facts": state.collected_facts
    }
    
    result = {
        "answer": f"æŠ±æ­‰ï¼Œåœ¨ä½¿ç”¨ {error_context['error_tool']} å·¥å…·æ—¶é‡åˆ°äº†é—®é¢˜ã€‚" +
                 "åŸºäºå·²æ”¶é›†çš„ä¿¡æ¯ï¼Œæˆ‘å¯ä»¥å‘Šè¯‰æ‚¨...",
        "confidence": 0.5,
        "error": error_context["error"]
    }
    
    msg = Message(
        role="error_handler",
        action="handle_error",
        content=result,
        metadata=error_context
    )
    state.add_message(msg)
    
    # æ¸…é™¤é”™è¯¯çŠ¶æ€
    state.metadata["last_error"] = None
    
    return state
